{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Toxicity Model With Natural Language Processing and Tokenization\n",
    "## Research Background\n",
    "> Comment toxicity is a major issue in online communication, as toxic comments can lead to a negative user experience and can contribute to the spread of hate speech. Deep learning models have been applied to the task of detecting toxicity in comments, using techniques such as natural language processing and machine learning.\n",
    "\n",
    "\n",
    "## Schema\n",
    "> ***CSV ––> Inputs(strings) ––> Labels(mullti-binary classification) AND (Tokenization (text vectorization layer in Keras) ––> embedded vector) ––> Deep Neural Network(lstm layers for working with sequence of embeddings) ––> Serialize into H5 Format ––> Integrate Model to Gradio App*** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model Implementation\n",
    ">\n",
    "> #### Types of labels\n",
    "> - toxic\n",
    ">\n",
    "> - severe_toxic\n",
    ">\n",
    "> - obscene\n",
    ">\n",
    "> - threat\n",
    ">\n",
    "> - insult\n",
    ">\n",
    "> - identity_hate\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for navigating file paths\n",
    "import os\n",
    "# for df \n",
    "import pandas as pd\n",
    "# deep learning framework \n",
    "import tensorflow as tf\n",
    "# array manipulation(wrap)\n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file \n",
    "\n",
    "# for our purposes will be using just train data for testing and validation\n",
    "df = pd.read_csv(\n",
    "    'jigsaw-toxic-comment-classification-challenge/train.csv.zip'\n",
    ")\n",
    "df.head()\n",
    "\n",
    "# see individual comment entries \n",
    "# df.iloc[7]['comment_text']\n",
    "# see toxicity stats for specified entry number\n",
    "# calls index of column 2 to end\n",
    "# df[df.columns[2:]].iloc[7]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for natural language processing \n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "# input\n",
    "X = df['comment_text']\n",
    "# output (.values to convert to numpy array)\n",
    "y = df[df.columns[2:]].values\n",
    "\n",
    "# number of words in the vocab \n",
    "MAX_FEATURES = 200000\n",
    "\n",
    "# init text vectorization layer \n",
    "vectorizer = TextVectorization( \n",
    "    max_tokens=MAX_FEATURES, \n",
    "    output_sequence_length=1800,\n",
    "    # specify word ––> integer \n",
    "    output_mode='int'\n",
    "    )\n",
    "# learn vocabulary \n",
    "vectorizer.adapt(X.values)\n",
    "\n",
    "# returns array of vocabulary\n",
    "vectorizer.get_vocabulary()\n",
    "\n",
    "# returns token array given string series\n",
    "# vectorizer('Hello world')[:2] # number of values(words)\n",
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   67,    11,     2, ...,     0,     0,     0],\n",
       "        [22832,  2780, 51387, ...,     0,     0,     0],\n",
       "        [  312,   104,     2, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    7,   654,    20, ...,     0,     0,     0],\n",
       "        [ 7132,     8,    55, ...,     0,     0,     0],\n",
       "        [   46,  4525,     2, ...,     0,     0,     0]]),\n",
       " array([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MCSHBAP - map, cache, shuffle, batch, prefetch (from_tensorflow_slices, list_file)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(16)\n",
    "# helps prevent bottlenecks \n",
    "dataset = dataset.prefetch(8)\n",
    "\n",
    "# fetch a batch\n",
    "dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Split into Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take and partition data \n",
    "train = dataset.take(int(len(dataset) * .7))\n",
    "# skip partitioned data and take \n",
    "val = dataset.skip(int(len(dataset) * .7)).take(int(len(dataset) * .2))\n",
    "# skip partitioned data and take\n",
    "test = dataset.skip(int(len(dataset) * .9)).take(int(len(dataset) * .1))\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          6400032   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               16640     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,491,686\n",
      "Trainable params: 6,491,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# api \n",
    "from keras.models import Sequential \n",
    "# layers \n",
    "from keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding\n",
    "\n",
    "# init sequential model\n",
    "model = Sequential()\n",
    "# create embedding layer\n",
    "model.add(Embedding(MAX_FEATURES + 1, 32))\n",
    "# parse LSTM bidirectionally in recurring nueral network\n",
    "model.add(Bidirectional(LSTM(32, activation = 'tanh')))\n",
    "# connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# final layer is 6 so we can output the 6 labels of toxicity\n",
    "# passing through sigmoid insures value is between 0 and 1 \n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# \n",
    "model.compile(loss='BinaryCrossentropy', optimizer = 'Adam')\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6981/6981 [==============================] - 6020s 862ms/step - loss: 0.0615 - val_loss: 0.0449\n"
     ]
    }
   ],
   "source": [
    "# low epoch count will impact accuracy, however a high one will drastically increase training time \n",
    "history = model.fit(train, epochs=1, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and val_loss metrics \n",
    "history.history\n",
    "\n",
    "# plot metrics\n",
    "# in our case 1 pass over the data does not gerenate sufficient data points\n",
    "# to do this we need to modify epochs to maybe 5 or 10\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch predictions\n",
    "batch = test.as_numpy_iterator().next()\n",
    "\n",
    "# expected string as integer for model \n",
    "input_text = vectorizer('I love You!')\n",
    "\n",
    "# predict\n",
    "# returns array whose index correspond to labels(i.e df.columns[2:])\n",
    "# model.predict(np.expand_dims(input_text,0))\n",
    "model.predict(np.array([input_text]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metric tools\n",
    "from keras.metrics import Precision, Recall, CategoricalAccuracy\n",
    "\n",
    "# allows us to update metrics as we iterate over batches\n",
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()\n",
    "\n",
    "for batch in test.as_numpy_iterator():\n",
    "    # unpack batch\n",
    "    X_true, y_true = batch \n",
    "    # make prediction similar to above\n",
    "    yhat = model.predict(X_true)\n",
    "    # flatten predictions to one vector\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    # update metrics\n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)\n",
    "\n",
    "# disp results\n",
    "print(f'Precision: {pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio App Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import demo tool\n",
    "import gradio as gr \n",
    "# can also load model if running on different server\n",
    "model.save('commentToxicity.h5')\n",
    "\n",
    "# basic input flow\n",
    "# tokenizing input\n",
    "comment = vectorizer('I love you')\n",
    "# prediction\n",
    "res = model.predict(np.expand_dims(comment,0))\n",
    "\n",
    "\n",
    "# func hooks into gradio model\n",
    "def score_comment(comment):\n",
    "    # tokenize input\n",
    "    vectorized_comment = vectorizer([comment])\n",
    "    # parse and feed model\n",
    "    results = model.predict(vectorized_comment)\n",
    "    \n",
    "    # iterate through columns\n",
    "    # thresold is >0.5 to return true \n",
    "    text = ''\n",
    "    for idx, col in enumerate(df.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(col, results[0][idx]>0.5)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# gradio inteface \n",
    "# flag element will create a .csv of flagged comments on user click\n",
    "interface = gr.Interface(\n",
    "    fn=score_comment, \n",
    "    inputs=gr.inputs.Textbox(lines=2, placeholder='input your comment ; shashi will rate your toxicity...'),\n",
    "    outputs='text')\n",
    "\n",
    "# enable/display public sharing                 \n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Conclusion\n",
    ">\n",
    "> This model is decent at classifying a given comment's toxicity, and it will only get better given more training time.\n",
    "> \n",
    "> However, it would reason that a pre-trained transformer would be far superior.\n",
    ">\n",
    "> Note that to arrive at this conclusion the model was initially trained for roughly an hour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9866e9086065cd29b29e156755c5205638517ac93a305727ff25729ade5ca316"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
